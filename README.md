# honk
An intelligent, albeit atechnical attempt at minimizing token utilization for LLMs; conceptualized by a human, designed by an LLM.
**Let me first address the idea, as it is in a formative state, and the manner by which it's elaboration will proceed.**
I'm an enthusiastic, and inderpendently motivated AI reasearcher/engineer/user, however it might be classified. My goal, as I currently recognize it is to enable myself to shorthand an LLM prompt to "compress" tokenized text (I do not have an intricate understanding of these topics; but understand that text characters generated or prompted are quantified as 'tokens' in a compute usage context; and less text amnounts to less necessary compute power). 

Methodology (theoretical) - Establish an algorithm or programming function (again, please be aware I'm not deeply familiar with the technical execution) that allows users of programs like ChatGPT, assuming said program is capable of realtime web querys / data scraping, to prefix a prompt with something in the scope of "goose:(link.url)" so that the within the subsequent text of the font they can write "honk:link.url" A1 B1 generative art" instead of "create a through-provoking article written in a professional tone by an optimistic user of AI in generative art" if the "link.url" leads to their (or another) publicly visible spreadsheet wherein A,1 = "create a throrough, throught provoking article" and B,1 = "written by an optimistic user of ai in"

I'm currently using various LLM models/interfaces/agents to best determine the ideal result, and will share my progress as it develops. All subsequent commits will be pursuant to transparency retentative of accredidation for "the honk algorithm" or as-of-yet undetermined designation. 
